# 网络

## 单工、半双工和全双工通信

- 单工数据传输只支持数据在一个方向上传输;
- 半双工数据传输允许数据在两个方向上传输，但是，在某一时刻，只允许数据在一个方向上传输，它实际上是一种切换方向的单工通信；
- 全双工数据通信允许数据同时在两个方向上传输，因此，全双工通信是两个单工通信方式的结合，它要求发送设备和接收设备都有独立的接收和发送能力。

## OSI 七层模型

- 应用层：各种应用软件，常用协议 HTTP，SSH，FTP
- 表示层：数据格式标识，基本压缩加密功能
- 会话层：控制应用程序之间会话能力，区分不同的进程
- 传输层：提供端对端的接口，TCP，UDP-
- 网络层：定义 IP 地址，定义路由功能，建立主机到主机的通信
- 数据链路层：将比特封装成数据帧并传递
- 物理层：网络连接介质，如网线、光缆，数据在其中以比特为单位传输，二进制的数据形式在物理媒体上传输数据

![OSI七层模型](/images/osi.webp)

## TCP/IP 网络模型

1. TCP/IP 模型是一系列网络协议的总称，这些协议的目的是使得计算机之间可以进行信息交换，

2. TCP/IP 模型五层架构从下到上分别是物理层，链路层，网络层，传输层，应用层

- 应用层：直接为应用进程提供服务。应用层协议定义的是应用进程间通讯和交互的规则，不同的应用有着不同的应用层协议，如 HTTP 协议（万维网服务）、FTP 协议（文件传输）、SMTP 协议（电子邮件）、DNS（域名查询）等
- 传输层：负责为两台主机中的进程提供通信服务，主要协议是 TCP、UDP
- 网络层：负责为两台主机提供通信服务，并通过选择合适的路由将数据传递到目标主机，主要协议是 IP 协议，
- 数据链路层：负责将网络层交下来的 IP 数据报封装成帧，并在链路的两个相邻节点间传送帧，每一帧都包含数据和必要的控制信息（如同步信息、地址信息、差错控制等）
- 物理层：确保数据可以在各种物理媒介上进行传输，为数据的传输提供可靠的环境。

![TCP/IP 网络模型](/images/TCP_IP.png)

**设备支持**：
![TCP/IP 网络模型设备](/images/equipment.webp)

**协议支持**：
![TCP/IP 网络模型协议](/images/protocol.webp)

## UDP

全称是用户数据报协议

UDP 协议是面向无连接的，也就是说不需要在正式传递数据之前先连接起双方。然后 UDP 协议只是数据报文的搬运工，不保证有序且不丢失的传递到对端，并且 UDP 协议也没有任何控制流量的算法，总的来说 UDP 相较于 TCP 更加的轻便

1. **面向无连接**：UDP 是不需要和 TCP 一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了，并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作
2. **传播方式**：UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能
3. **面向报文**：发送方的 UDP 对应用程序交下来的报文，在添加首部后就向下交付 IP 层。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。因此，应用程序必须选择合适大小的报文
4. **不可靠性**：

   不可靠性体现在无连接上，通信都不需要建立连接，想发就发，这样的情况肯定不可靠。并且收到什么数据就传递什么数据，并且也不会备份数据，发送数据也不会关心对方是否已经正确接收到数据了。

   再者网络环境时好时坏，但是 UDP 因为没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP。

   总结：

   - 不保证消息交付：不确认，不重传，无超时
   - 不保证交付顺序：不设置包序号，不重排，不会发生队首阻塞
   - 不跟踪连接状态：不必建立连接或重启状态机
   - 不进行拥塞控制：不内置客户端或网络反馈机制

5. **头部开销小，传输数据报文时是很高效的**

   UDP 头部包含了以下几个数据：

   - 两个十六位的端口号，分别为源端口（可选字段）和目标端口
   - 整个数据报文的长度
   - 整个数据报文的检验和（IPv4 可选字段），该字段用于发现头部信息和数据中的错误

   因此 UDP 的头部开销小，只有 8 字节，相比 TCP 的至少 20 字节要少得多，在传输数据报文时是很高效的

## TCP

TCP 的全称是传输控制协议，是一种面向连接的、可靠的、基于字节流的传输层通信协议。TCP 是面向连接的、可靠的流协议。流就是指不间断的数据结构。

特点：

1. **面向连接**

   面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础。

2. **仅支持单播传输**

   每条 TCP 传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式。

3. **面向字节流**

   TCP 不像 UDP 一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以字节流方式进行传输。

4. **可靠传输**

   对于可靠传输，判断丢包、误码靠的是 TCP 的段编号以及确认号。TCP 为了保证报文传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）将会被重传。

5. **提供拥塞控制**

   当网络出现拥塞的时候，TCP 能够减小向网络注入数据的速率和数量，缓解拥塞。

6. **提供全双工通信**

   TCP 允许通信双方的应用程序在任何时候都能发送数据，因为 TCP 连接的两端都设有缓存，用来临时存放双向通信的数据。当然，TCP 可以立即发送一个数据段，也可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于 MSS）

### 重传机制

由于 TCP 的下层网络（网络层）可能出现**丢失、重复或失序**的情况，TCP 协议提供可靠数据传输服务。为保证数据传输的正确性，TCP 会重传其认为已丢失（包括报文中的比特错误）的包。TCP 使用两套独立的机制来完成重传，一是**基于时间**，二是**基于确认信息**。

TCP 在发送一个数据之后，就开启一个定时器，若是在这个时间内没有收到发送数据的 ACK 确认报文，则对该报文进行重传，在达到一定次数还没有成功时放弃并发送一个复位信号。

### 拥塞控制机制

TCP 的拥塞控制机制主要是以下四种机制：

1. 慢启动（慢开始）
   - 在开始发送的时候设置 cwnd = 1（cwnd 指的是拥塞窗口）
   - 思路：开始的时候不要发送大量数据，而是先测试一下网络的拥塞程度，由小到大增加拥塞窗口的大小。
   - 为了防止 cwnd 增长过大引起网络拥塞，设置一个慢开始门限(ssthresh 状态变量)
     - 当 cnwd < ssthresh，使用慢开始算法
     - 当 cnwd = ssthresh，既可使用慢开始算法，也可以使用拥塞避免算法
     - 当 cnwd > ssthresh，使用拥塞避免算法
2. 拥塞避免
   - 拥塞避免未必能够完全避免拥塞，是说在拥塞避免阶段将拥塞窗口控制为按线性增长，使网络不容易出现阻塞。
   - 思路： 让拥塞窗口 cwnd 缓慢的增大，即每经过一个返回时间 RTT 就把发送方的拥塞控制窗口加一
   - 无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞，就把慢开始门限设置为出现拥塞时的发送窗口大小的一半。然后把拥塞窗口设置为 1，执行慢开始算法。
3. 快速重传
   - 快重传要求接收方在收到一个失序的报文段后就立即发出重复确认(为的是使发送方及早知道有报文段没有到达对方)。发送方只要连续收到三个重复确认就立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。
   - 由于不需要等待设置的重传计时器到期，能尽早重传未被确认的报文段，能提高整个网络的吞吐量
4. 快速恢复
   - 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把 ssthresh 门限减半。但是接下去并不执行慢开始算法。
   - 考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将 cwnd 设置为 ssthresh 的大小，然后执行拥塞避免算法。

### 流量控制机制

一般来说，流量控制就是为了让发送方发送数据的速度不要太快，要让接收方来得及接收。TCP 采用大小可变的滑动窗口进行流量控制，窗口大小的单位是字节。这里说的窗口大小其实就是每次传输的数据大小。

- 当一个连接建立时，连接的每一端分配一个缓冲区来保存输入的数据，并将缓冲区的大小发送给另一端。
- 当数据到达时，接收方发送确认，其中包含了自己剩余的缓冲区大小。（剩余的缓冲区空间的大小被称为窗口，指出窗口大小的通知称为窗口通告 。接收方在发送的每一确认中都含有一个窗口通告。）
- 如果接收方应用程序读数据的速度能够与数据到达的速度一样快，接收方将在每一确认中发送一个正的窗口通告。
- 如果发送方操作的速度快于接收方，接收到的数据最终将充满接收方的缓冲区，导致接收方通告一个零窗口 。发送方收到一个零窗口通告时，必须停止发送，直到接收方重新通告一个正的窗口。

### 可靠传输机制

TCP 的可靠传输机制是基于连续 ARQ 协议和滑动窗口协议的。

TCP 协议在发送方维持了一个发送窗口，发送窗口以前的报文段是已经发送并确认了的报文段，发送窗口中包含了已经发送但 未确认的报文段和允许发送但还未发送的报文段，发送窗口以后的报文段是缓存中还不允许发送的报文段。当发送方向接收方发 送报文时，会依次发送窗口内的所有报文段，并且设置一个定时器，这个定时器可以理解为是最早发送但未收到确认的报文段。 如果在定时器的时间内收到某一个报文段的确认回答，则滑动窗口，将窗口的首部向后滑动到确认报文段的后一个位置，此时如 果还有已发送但没有确认的报文段，则重新设置定时器，如果没有了则关闭定时器。如果定时器超时，则重新发送所有已经发送 但还未收到确认的报文段，并将超时的间隔设置为以前的两倍。当发送方收到接收方的三个冗余的确认应答后，这是一种指示， 说明该报文段以后的报文段很有可能发生丢失了，那么发送方会启用快速重传的机制，就是当前定时器结束前，发送所有的已发 送但确认的报文段。

接收方使用的是累计确认的机制，对于所有按序到达的报文段，接收方返回一个报文段的肯定回答。如果收到了一个乱序的报文 段，那么接方会直接丢弃，并返回一个最近的按序到达的报文段的肯定回答。使用累计确认保证了返回的确认号之前的报文段都 已经按序到达了，所以发送窗口可以移动到已确认报文段的后面。

发送窗口的大小是变化的，它是由接收窗口剩余大小和网络中拥塞程度来决定的，TCP 就是通过控制发送窗口的长度来控制报文 段的发送速率。

但是 TCP 协议并不完全和滑动窗口协议相同，因为许多的 TCP 实现会将失序的报文段给缓存起来，并且发生重传时，只会重 传一个报文段，因此 TCP 协议的可靠传输机制更像是窗口滑动协议和选择重传协议的一个混合体。

### 三次握手

三次握手（Three-way Handshake）其实就是指建立一个 TCP 连接时，需要客户端和服务器总共发送 3 个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。

刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态。

- 第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN，此时客户端处于 SYN_SEND 状态。
  > 首部的同步位 SYN=1，初始序号 seq=x，SYN=1 的报文段不能携带数据，但要消耗掉一个序号。
- 第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN。同时会把客户端的 ISN + 1 作为 ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于 SYN_REVD 的状态。
  > 在确认报文段中 SYN=1，ACK=1，确认号 ack=x+1，初始序号 seq=y
- 第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 ESTABLISHED 状态。服务器收到 ACK 报文之后，也处于 ESTABLISHED 状态，此时，双方已建立起了连接。
  > 确认报文段 ACK=1，确认号 ack=y+1，序号 seq=x+1（初始为 seq=x，第二个报文段所以要+1），ACK 报文段可以携带数据，不携带数据则不消耗序号。

#### 为什么需要三次，两次和四次可以吗？

- 为了确认双方的接收能力和发送能力都正常，防止出现失效的连接请求报文段被服务端接收的情况，从而产生错误
- 如果是用两次握手，则会出现下面这种情况：
  > 如客户端发出连接请求，但因连接请求报文丢失而未收到确认，于是客户端再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接，客户端共发出了两个连接请求报文段，其中第一个丢失，第二个到达了服务端，但是第一个丢失的报文段只是在某些网络结点长时间滞留了，延误到连接释放以后的某个时间才到达服务端，此时服务端误认为客户端又发出一次新的连接请求，于是就向客户端发出确认报文段，同意建立连接，不采用三次握手，只要服务端发出确认，就建立新的连接了，此时客户端忽略服务端发来的确认，也不发送数据，则服务端一致等待客户端发送数据，浪费资源。
- 为什么不是四次握手呢
  > 大家应该知道通信中著名的蓝军红军约定， 这个例子说明， 通信不可能 100%可靠， 而上面的三次握手已经做好了通信的准备工作， 再增加握手， 并不能显著提高可靠性， 而且也没有必要。

**简单来说就是以下三步：**

- 第一次握手：客户端向服务端发送连接请求报文段。该报文段中包含自身的数据通讯初始序号。请求发送后，客户端便进入 SYN-SENT 状态。
- 第二次握手：服务端收到连接请求报文段后，如果同意连接，则会发送一个应答，该应答中也会包含自身的数据通讯初始序号，发送完成后便进入 SYN-RECEIVED 状态。
- 第三次握手：当客户端收到连接同意的应答后，还要向服务端发送一个确认报文。客户端发完这个报文段后便进入 ESTABLISHED 状态，服务端收到这个应答后也进入 ESTABLISHED 状态，此时连接建立成功。

TCP 三次握手的建立连接的过程就是相互确认初始序号的过程，告诉对方，什么样序号的报文段能够被正确接收。 第三次握手的作用是客户端对服务器端的初始序号的确认。如果只使用两次握手，那么服务器就没有办法知道自己的序号是否 已被确认。同时这样也是为了防止失效的请求报文段被服务器接收，而出现错误的情况。

### 四次挥手

最开始的时候，客户端和服务器都是处于 ESTABLISHED 状态，假设客户端主动关闭，服务器被动关闭，四次挥手的过程如下：

- 第一次挥手： 客户端会发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于 FIN_WAIT1 状态。
  > 即发出连接释放报文段（FIN=1，序号 seq=u），并停止再发送数据，主动关闭 TCP 连接，进入 FIN_WAIT1（终止等待 1）状态，等待服务端的确认。
- 第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT 状态。
  > 即服务端收到连接释放报文段后即发出确认报文段（ACK=1，确认号 ack=u+1，序号 seq=v），服务端进入 CLOSE_WAIT（关闭等待）状态，此时的 TCP 处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入 FIN_WAIT2（终止等待 2）状态，等待服务端发出的连接释放报文段。
- 第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。
  > 即服务端没有要向客户端发出的数据，服务端发出连接释放报文段（FIN=1，ACK=1，序号 seq=w，确认号 ack=u+1），服务端进入 LAST_ACK（最后确认）状态，等待客户端的确认。
- 第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 +1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态，服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。
  > 即客户端收到服务端的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），客户端进入 TIME_WAIT（时间等待）状态。此时 TCP 未释放掉，需要经过时间等待计时器设置的时间 2MSL 后，客户端才进入 CLOSED 状态。

#### 为什么客户端最后还要等待 2MSL

`MSL（Maximum Segment Lifetime）`，TCP 允许不同的实现可以设置不同的 MSL 值。

1. 保证客户端发送的最后一个 ACK 报文能够到达服务器，因为这个 ACK 报文可能丢失，站在服务器的角度看来，我已经发送了 FIN+ACK 报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个 2MSL 时间段内收到这个重传的报文，接着给出回应报文，并且会重启 2MSL 计时器。
2. 防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个 2MSL 时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。

#### 为什么建立连接是三次握手，关闭连接确是四次挥手呢？

建立连接的时候， 服务器在 LISTEN 状态下，收到建立连接请求的 SYN 报文后，把 ACK 和 SYN 放在一个报文里发送给客户端，其中 ACK 报文是用来应答的，SYN 报文是用来同步的。

而关闭连接时，服务器收到对方的 FIN 报文时，很可能并不会立即关闭 SOCKET，所以只能先回复一个 ACK 报文，告诉客户端，“你发的 FIN 报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送 FIN 报文。因此己方 ACK 和 FIN 一般都会分开发送，从而导致多了一次。

**简单来说就是以下四步：**

- 第一次挥手：若客户端认为数据发送完成，则它需要向服务端发送连接释放请求。
- 第二次挥手：服务端收到连接释放请求后，会告诉应用层要释放 TCP 链接。然后会发送 ACK 包，并进入 CLOSE_WAIT 状态，此时表明客户端到服务端的连接已经释放，不再接收客户端发的数据了。但是因为 TCP 连接是双向的，所以服务端仍旧可以发送数据给客户端。
- 第三次挥手：服务端如果此时还有没发完的数据会继续发送，完毕后会向客户端发送连接释放请求，然后服务端便进入 LAST-ACK 状态。
- 第四次挥手：客户端收到释放请求后，向服务端发送确认应答，此时客户端进入 TIME-WAIT 状态。该状态会持续 2MSL（最大段生存期，指报文段在网络中生存的时间，超时会被抛弃） 时间，若该时间段内没有服务端的重发请求的话，就进入 CLOSED 状态。当服务端收到确认应答后，也便进入 CLOSED 状态。

TCP 使用四次挥手的原因是因为 TCP 的连接是全双工的，所以需要双方分别释放到对方的连接，单独一方的连接释放，只代 表不能再向对方发送数据，连接处于的是半释放的状态。

最后一次挥手中，客户端会等待一段时间再关闭的原因，是为了防止发送给服务器的确认报文段丢失或者出错，从而导致服务器 端不能正常关闭。

### TCP 粘包

默认情况下, TCP 连接会启⽤**延迟传送算法 (Nagle 算法)**, 在数据发送之前缓存他们. 如果短时间有多个数据发送, 会缓冲到⼀起作⼀次发送 (缓冲⼤⼩⻅ socket.bufferSize ), 这样可以减少 IO 消耗提⾼性能. 

如果是传输⽂件的话, 那么根本不⽤处理粘包的问题, 来⼀个包拼⼀个包就好了。但是如果是多条消息, 或者是别的⽤途的数据那么就需要处理粘包. 

下面看⼀个例⼦, 连续调⽤两次 send 分别发送两段数据 data1 和 data2, 在接收端有以下⼏种常⻅的情况: 

1. 先接收到 data1, 然后接收到 data2 . 
2. 先接收到 data1 的部分数据, 然后接收到 data1 余下的部分以及 data2 的全部. 
3. 先接收到了 data1 的全部数据和 data2 的部分数据, 然后接收到了 data2 的余下的数据. 
4. ⼀次性接收到了 data1 和 data2 的全部数据. 

其中的 BCD 就是我们常⻅的粘包的情况. ⽽对于处理粘包的问题, 常⻅的解决⽅案有: 
- **多次发送之前间隔⼀个等待时间**：只需要等上⼀段时间再进⾏下⼀次 send 就好, 适⽤于交互频率特别低的场景. 缺点也很明显, 对于⽐较频繁的场景⽽⾔传输效率实在太低，不过⼏乎不⽤做什么处理. 
- **关闭 Nagle 算法**：关闭 Nagle 算法, 在 Node.js 中你可以通过 socket.setNoDelay() ⽅法来关闭 Nagle 算法, 让每⼀次 send 都不缓冲直接发送。该⽅法⽐较适⽤于每次发送的数据都⽐较⼤ (但不是⽂件那么⼤), 并且频率不是特别⾼的场景。如果是每次发送的数据量⽐较⼩, 并且频率特别⾼的, 关闭 Nagle 纯属⾃废武功。另外, 该⽅法不适⽤于⽹络较差的情况, 因为 Nagle 算法是在服务端进⾏的包合并情况, 但是如果短时间内客户端的⽹络情况不好, 或者应⽤层由于某些原因不能及时将 TCP 的数据 recv, 就会造成多个包在客户端缓冲从⽽粘包的情况。 (如果是在稳定的机房内部通信那么这个概率是⽐较⼩可以选择忽略的)  
- **进⾏封包/拆包**：封包/拆包是⽬前业内常⻅的解决⽅案了。即给每个数据包在发送之前, 于其前/后放⼀些有特征的数据, 然后收到数据的时 候根据特征数据分割出来各个数据包。

#### 为什么udp不会粘包？ 
- TCP协议是⾯向流的协议，UDP是⾯向消息的协议。UDP段都是⼀条消息，应⽤程序必须以消息为单位提取数据，不能⼀次提取任意字节的数据 
- UDP具有保护消息边界，在每个UDP包中就有了消息头（消息来源地址，端⼝等信息），这样对于接收端来说就容易进⾏区分处理了。传输协议把数据当作⼀条独⽴的消息在⽹上传输，接收端只能接收独⽴的消息。接收端⼀次只能接收发送端发出的⼀个数据包,如果⼀次接受数据的⼤⼩⼩于发送端⼀次发送的数据⼤⼩，就会丢失⼀部分数据，即使丢失，接受端也不会分两次去接收。

## DNS

- 概念： DNS 是域名系统 (Domain Name System) 的缩写，提供的是一种主机名到 IP 地址的转换服务，就是我们常说的域名系统。它是一个由分层的 DNS 服务器组成的分布式数据库，是定义了主机如何查询这个分布式数据库的方式的应用层协议。能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的 IP 数串。
- 作用： 将域名解析为 IP 地址，客户端向 DNS 服务器（DNS 服务器有自己的 IP 地址）发送域名查询请求，DNS 服务器告知客户机 Web 服务器的 IP 地址。
- 优化： 缓存、负载均衡

**DNS 占用 53 号端口，同时使用 TCP 和 UDP 协议。**

1. 在区域传输的时候使用 TCP 协议
   - 辅域名服务器会定时（一般 3 小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，会执行一次区域传送，进行数据同步。区域传送使用 TCP 而不是 UDP，因为数据同步传送的数据量比一个请求应答的数据量要多得多。
   - TCP 是一种可靠连接，保证了数据的准确性。
2. 在域名解析的时候使用 UDP 协议
   - 客户端向 DNS 服务器查询域名，一般返回的内容都不超过 512 字节，用 UDP 传输即可。不用经过三次握手，这样 DNS 服务器负载更低，响应更快。理论上说，客户端也可以指定向 DNS 服务器查询时用 TCP，但事实上，很多 DNS 服务器进行配置的时候，仅支持 UDP 查询包。

DNS 服务器解析域名的过程：

1. 首先会在**浏览器的缓存**中查找对应的 IP 地址，如果查找到直接返回，若找不到继续下一步
2. 将请求发送给**本地 DNS 服务器**，在本地域名服务器缓存中查询，如果查找到，就直接将查找结果返回，若找不到继续下一步
3. 本地 DNS 服务器向**根域名服务器**发送请求，根域名服务器会返回一个所查询域的顶级域名服务器地址
4. 本地 DNS 服务器向**顶级域名服务器**发送请求，接受请求的服务器查询自己的缓存，如果有记录，就返回查询结果，如果没有就返回相关的下一级的权威域名服务器的地址
5. 本地 DNS 服务器向**权威域名服务器**发送请求，域名服务器返回对应的结果
6. 本地 DNS 服务器将返回结果保存在缓存中，便于下次使用
7. 本地 DNS 服务器将返回结果返回给浏览器

\*_3\4\5 是迭**代查询**，2\6\7 是**递归查询**_

## CDN

CDN 的全称是 Content Delivery Network，即内容分发网络。

CDN 的基本原理是广泛采用各种缓存服务器，将这些缓存服务器分布到用户访问相对集中的地区或网络中，在用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应。

对于 CDN 加载静态资源需要注意 CDN 域名要与主站不同，否则每次请求都会带上主站的 Cookie。

**作用：**

CDN 一般会用来托管 Web 资源（包括文本、图片和脚本等），可供下载的资源（媒体文件、软件、文档等），应用程序（门户网站等）。使用 CDN 来加速这些资源的访问

1. 在性能方面，引入 CDN 的作用在于：

- 用户收到的内容来自最近的数据中心，延迟更低，内容加载更快
- 部分资源请求分配给了 CDN，减少了服务器的负载

2. 在安全方面，CDN 有助于防御 DDoS、MITM 等网络攻击：

- 针对 DDoS：通过监控分析异常流量，限制其请求频率
- 针对 MITM：从源服务器到 CDN 节点到 ISP（Internet Service Provider），全链路 HTTPS 通信

除此之外，CDN 作为一种基础的云服务，同样具有资源托管、按需扩展（能够应对流量高峰）等方面的优势。

## HTTP

> 超文本传输协议（hypertext transfer protocol），连接客户端，网关和服务器，HTTP 是一个基于 TCP/IP 通信协议来传递数据

**优点**：

- 支持客户端/服务器模式
- 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。由于 HTTP 协议简单，使得 HTTP 服务器的程序规模小，因而通信速度很快。
- 无连接：无连接就是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接，采用这种方式可以节省传输时间。（可通过设置自身属性 Keep-Alive 解决）
- 无状态：HTTP 协议是无状态协议，这里的状态是指通信过程的上下文信息。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能会导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就比较快。（只能通过 cookie 和 session 来做贮存）
- 灵活（媒体独立）：只要客户端和服务器知道如何处理的数据内容，HTTP 允许传输任意类型的数据对象。正在传输的类型由 Content-Type 加以标记。

**缺点**：

- 无状态：HTTP 是一个无状态的协议，HTTP 服务器不会保存关于客户的任何信息。
- 明文传输：协议中的报文使用的是文本形式，这就直接暴露给外界，不安全。
- 不安全
  - 通信使用明文（不加密），内容可能会被窃听；
  - 不验证通信方的身份，因此有可能遭遇伪装；
  - 无法证明报文的完整性，所以有可能已遭篡改；

### 请求报文

一个 HTTP 请求报文由请求行（request line）、请求头（header）、空行和请求数据 4 个部分组成

- **请求行组成**：

  - 请求方法：
    - GET: 向服务器获取数据；
    - POST：将实体提交到指定的资源，通常会造成服务器资源的修改；
    - PUT：上传文件，更新数据；
    - DELETE：删除服务器上的对象；
    - HEAD：获取报文首部，与 GET 相比，不返回报文主体部分；
    - OPTIONS：询问支持的请求方法，用来跨域请求；
    - CONNECT：要求在与代理服务器通信时建立隧道，使用隧道进行 TCP 通信；
    - TRACE: 回显服务器收到的请求，主要⽤于测试或诊断；
    - PATCH：对资源进行部分修改；
  - URL
  - 协议版本

- **OPTIONS 请求方法及使用场景**
  OPTIONS 是除了 GET 和 POST 之外的其中一种 HTTP 请求方法。

  OPTIONS 方法是用于请求获得由 Request-URI 标识的资源在请求/响应的通信过程中可以使用的功能选项。通过这个方法，客户端可以在**采取具体资源请求之前，决定对该资源采取何种必要措施，或者了解服务器的性能**。该请求方法的响应不能缓存。

  OPTIONS 请求方法的主要**用途**有两个：

  - 获取服务器支持的所有 HTTP 请求方法；
  - 用来检查访问权限。例如：在进行 CORS 跨域资源共享时，对于复杂请求，就是使用 OPTIONS 方法发送嗅探请求，以判断是否有对指定资源的访问权限。

- **常见请求头**：
  - Accept：客户端可识别的内容类型列表（浏览器的用户代理字符串）
  - Accept-Charset:浏览器能够显示的字符集
  - Accept-Encoding：浏览器能够处理的压缩编码
  - Accept-Language：浏览器当前设置的语言
  - Connection：浏览器与服务器之间连接的类型
  - Content-Type：请求体的 MIME 类型 （用于 POST 和 PUT 请求中）。如：Content-Type: application/x-www-form-urlencoded
  - Cookie：当前页面设置的任何 Cookie
  - Host：请求的主机名，允许多个域名同处一个 IP 地址，即虚拟主机
  - Referer：发出请求的页面的 URL
  - User-Agent：产生请求的浏览器类型。

### 响应报文

包括：响应行、响应头、空行、响应正文

- **常见相应头**：

  - Date：表示消息发送的时间，时间的描述格式由 rfc822 定义
  - server:服务器名称
  - Connection：浏览器与服务器之间连接的类型
  - Cache-Control：控制 HTTP 缓存
  - content-type:表示后面的文档属于什么 MIME 类型

- **content-type 常见类型**：
  - application/x-www-form-urlencoded：浏览器的原生 form 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。该种方式提交的数据放在 body 里面，数据按照 key1=val1&key2=val2 的方式进行编码，key 和 val 都进行了 URL 转码。
  - multipart/form-data：该种方式也是一个常见的 POST 提交方式，通常表单上传文件时使用该种方式。
  - application/json：服务器消息主体是序列化后的 JSON 字符串。
  - text/xml：该种方式主要用来提交 XML 格式的数据。

### HTTP1.1

- **连接方面**，http1.0 默认使用非持久连接，而 http1.1 默认使用持久连接。http1.1 通过使用持久连接(管道 pipeline 网络传输)来使多个 http 请求复用同一个 TCP 连接，以此来避免使用非持久连接时每次需要建立连接的时延。
- **资源请求方面**，在 http1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，http1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
- **缓存方面**，在 http1.0 中主要使用 header 里的 If-Modified-Since、Expires 来做为缓存判断的标准，http1.1 则引入了更多的缓存控制策略，例如 Etag、If-Unmodified-Since、If-Match、If-None-Match 等更多可供选择的缓存头来控制缓存策略。
- http1.1 中**新增了 host 字段**，用来指定服务器的域名。http1.0 中认为每台服务器都绑定一个唯一的 IP 地址，因此，请求消息中的 URL 并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机，并且它们共享一个 IP 地址。因此有了 host 字段，这样就可以将请求发往到同一台服务器上的不同网站。
- http1.1 相对于 http1.0 还新增了很多**请求方法**，如 PUT、HEAD、OPTIONS 等。

### HTTP2

- **二进制协议**：HTTP/2 是一个二进制协议。在 HTTP/1.1 版中，报文的头信息必须是文本（ASCII 编码），数据体可以是文本，也可以是二进制。HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为"帧"，可以分为头信息帧和数据帧。 帧的概念是它实现多路复用的基础。
- **多路复用**：HTTP/2 实现了多路复用，HTTP/2 仍然复用 TCP 连接，但是在一个连接里，客户端和服务器都可以同时发送多个请求或回应，而且不用按照顺序一一发送，这样就避免了"队头堵塞"的问题。

  \*_在 HTTP 1 下，浏览器对一个域名下最大 TCP 连接数为 6，可以用多域名部署解决，这样可以提高同时请求的数目。_

- **数据流**：HTTP/2 使用了数据流的概念，因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的请求。因此，必须要对数据包做标记，指出它属于哪个请求。HTTP/2 将每个请求或回应的所有数据包，称为一个数据流。每个数据流都有一个独一无二的编号。数据包发送时，都必须标记数据流 ID ，用来区分它属于哪个数据流。
- **头信息压缩**：HTTP/2 实现了头信息压缩，由于 HTTP 1.1 协议不带状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如 Cookie 和 User Agent ，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。HTTP/2 对这一点做了优化，引入了头信息压缩机制。一方面，头信息使用 gzip 或 compress 压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就能提高速度了。

  HTTP2 的头部压缩是**HPACK 算法**。在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，采用哈夫曼编码来压缩整数和字符串，可以达到 50%~90%的高压缩率。

  具体来说:

  - 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键值对，对于相同的数据，不再通过每次请求和响应发送；
  - 首部表在 HTTP/2 的连接存续期内始终存在，由客户端和服务器共同渐进地更新；
  - 每个新的首部键值对要么被追加到当前表的末尾，要么替换表中之前的值

  例如下图中的两个请求， 请求一发送了所有的头部字段，第二个请求则只需要发送差异数据，这样可以减少冗余数据，降低开销。

  ![头部压缩](/images/hpack.png)

- **服务器推送**：HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送。使用服务器推送提前给客户端推送必要的资源，这样就可以相对减少一些延迟时间。这里需要注意的是 http2 下服务器主动推送的是静态资源，和 WebSocket 以及使用 SSE 等方式向客户端发送即时数据的推送是不同的。

### 队头堵塞

队头阻塞是由 HTTP 基本的“请求 - 应答”模型所导致的。HTTP 规定报文必须是“一发一收”，这就形成了一个先进先出的“串行”队列。队列里的请求是没有优先级的，只有入队的先后顺序，排在最前面的请求会被最优先处理。如果队首的请求因为处理的太慢耽误了时间，那么队列里后面的所有请求也不得不跟着一起等待，结果就是其他的请求承担了不应有的时间成本，造成了队头堵塞的现象。

### HTTP3

TCP 改基于 UDP 的 QUIC，新增功能：多路复用、0-RTT、使用 TLS1.3 加密、流量控制、有序交付、重传等

1. 流量控制、传输可靠性功能：QUIC 在 UDP 的基础上增加了一层来保证数据传输可靠性，它提供了数据包重传、拥塞控制、以及其他一些 TCP 中的特性。
2. 集成 TLS 加密功能：目前 QUIC 使用 TLS1.3，减少了握手所花费的 RTT 数。
3. 多路复用：同一物理连接上可以有多个独立的逻辑数据流，实现了数据流的单独传输，解决了 TCP 的队头阻塞问题。
4. 快速握手：由于基于 UDP，可以实现使用 0 ~ 1 个 RTT 来建立连接

特性：

1. 连接迁移
   tcp 里面的四元组，一条 tcp 的唯一性标识，是由源 IP，源端口，目的 IP,目的端口，四元组标识。源 IP，源端口一般比较稳定，但是目的 IP,目的端口会由于网络元素等原因发生改变，一旦改变，那么此条 tcp 连接就会断开。
   由于 QUIC 基于 UDP 协议，所以一条 UDP 协议不再由四元组标识，而是以客户端随机产生的一个 64 位数字作为 ID 标识。只要 ID 不变，那么这条 UDP 就会存在，维持连接，上层业务逻辑就感受不到变化。
2. 无队头阻塞
   http2.0 的多路复用正好解决了 http 层的队头阻塞，但是 tcp 的队头阻塞依然存在。因为当数据包超时确认或者丢失，会等待重传，因此会阻塞当前窗口向右滑动，造成阻塞。
   而 QUIC 是基于 udp 的，创新点在于 QUIC 依靠一个严格的单调递增的 packet 序列，一个数据包里面还会有 streamID 和 streamoffset 偏移量，即使中途发生丢包或者超时确认，后面的数据包不会等待，等到接收完之后根据 ID 和 offset 即可完成重新拼装，从而避免了这种问题
3. 自定义的佣塞控制
   tcp 协议是在传输层，默认存在于系统中，而 QUIC 在应用层，当想要依据实际情况来重定义拥塞算法的时候，QUIC 显然更加灵活。Google 提出了 cubic 和 newreno 提供了许多可供编程的接口。当然，和 tcp 一样，也是默认采用 cubic 算法。
4. 前向安全和前向纠错
   Google 给 QUIC 加上了这个机制：每发送一组数据之后，就对这组数据进行异或运算（效率高），并将结果也发送出去，那么接收方就有两份数据版本，可以对初始数据进行纠错和校验。以此保证了可靠性。

### 状态码

- 1xx (Informational): 收到请求，正在处理
- 2xx (Successful): 该请求已成功收到，理解并接受
  - 200 OK：客户端发送给服务器的请求被正常处理并返回
  - 204 No content，表示请求成功，但响应报文不含实体的主体部分
  - 205 Reset Content，表示请求成功，但响应报文不含实体的主体部分，但是与 204 响应不同在于要求请求方重置内容
  - 206 Partial Content，进行范围请求
- 3xx (Redirection): 重定向
  - 301 Moved Permanently：永久重定向，请求的网页已永久移动到新位置。 服务器返回此响应时，会自动将请求者转到新位置
  - 302 Moved Permanently：临时重定向，请求的网页已临时移动到新位置。服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求
  - 303 see other，表示资源存在着另一个 URL，应使用 `GET` 方法获取资源，会把`POST`请求变为`GET`请求进⾏重定向。
  - 304 Not Modified：未修改，自从上次请求后，请求的网页未修改过。服务器返回此响应时，不会返回网页内容
    - 产生较多 304 状态码的原因：
      - 页面更新周期长或不更新
      - 纯静态页面或强制生成静态 html
    - 出现过多会造成以下问题：
      - 网站快照停止；
      - 收录减少；
      - 权重下降。
  - 307 temporary redirect，临时重定向，和 302 含义类似，但是期望客户端保持请求方法不变向新的地址发出请求，不会从`POST`变为`GET`
- 4xx (Client Error): 该请求包含错误的语法或不能为完成
  - 400 Bad Request：错误请求，服务器不理解请求的语法，常见于客户端传参错误
  - 401 Unauthorized：未授权，表示发送的请求需要有通过 HTTP 认证的认证信息，常见于客户端未登录
  - 403 Forbidden：禁止，服务器拒绝请求，常见于客户端权限不足
  - 404 Not Found：未找到，服务器找不到对应资源
- 5xx (Server Error): 服务器错误
  - 500 Inter Server Error：服务器内部错误，服务器遇到错误，无法完成请求
  - 501 Not Implemented：尚未实施，服务器不具备完成请求的功能
  - 502 Bad Gateway：作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。
  - 503 service unavailable：服务不可用，服务器目前无法使用（处于超载或停机维护状态）。通常是暂时状态

## GET/POST

1. GET 参数通过 url 传递，post 放在 request body 中。
2. GET 请求在 url 中传递的参数是有长度限制的（2083 字节(2K+35)），而 POST 没有。
3. GET 比 POST 更不安全，因为参数直接暴露在 url 中，所以不能用来传递敏感信息。
4. POST 的参数传递支持更多的数据类型
5. GET 请求只能进行 url 编码，而 POST 支持多种编码方式
6. GET 请求会浏览器主动缓存，而 POST 不会。
7. GET 请求参数会被完整保留在浏览历史记录里，而 POST 中的参数不会被保留。
8. GET 和 POST 本质上就是 TCP 链接，并无差别。但是由于 HTTP 的规定和浏览器/服务器的限制，导致他们在应用过程中体现出一些不同。

- 应用场景：
  - GET 多用于无副作用，幂等的场景，例如搜索关键字。
  - POST 多用于副作用，不幂等的场景，例如注册
- 副作用：对服务器上的资源做改变
- 幂等：发送 M 和 N 次请求（两者不相同且都大于 1），服务器上资源的状态一致

## HTTPS

> 超文本传输安全协议（Hypertext Transfer Protocol Secure，简称：HTTPS）是一种通过计算机网络进行安全通信的传输协议。

HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 来加密数据包。HTTPS 开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。

区别：

- HTTPS 使用 443 端口，而 HTTP 使用 80
- HTTPS 需要申请 SSL 证书，功能越强大的证书费用越高，且需要绑定 IP，不能再同一个 IP 上绑定多个域名
- HTTP 是超文本传输协议，是明文传输；HTTPS 是经过 SSL 加密的协议，传输更安全
- HTTPS 比 HTTP 慢，因为 HTTPS 除了 TCP 握手的三个包，还要加上 SSL 握手的九个包，增加页面的加载时间
- HTTPS 需要做服务器和客户端双方的加密个解密处理，耗费更多服务器资源，过程复杂，支持访客稍多的网站需要投入更大的成本

HTTP 协议采用明文传输信息，存在信息窃听、信息篡改和信息劫持的风险，而协议 TLS/SSL 具有身份验证、信息加密和完整性校验的功能，可以避免此类问题发生。

安全层的主要职责就是对发起的 HTTP 请求的数据进行加密操作 和 对接收到的 HTTP 的内容进行解密操作

## TLS/SSL 的工作原理

> TLS/SSL 全称安全传输层协议（Transport Layer Security）, 是介于 TCP 和 HTTP 之间的一层安全协议，不影响原有的 TCP 协议和 HTTP 协议，所以使用 HTTPS 基本上不需要对 HTTP 页面进行太多的改造。

TLS/SSL 的功能实现主要依赖三类基本算法：散列函数 hash、对称加密、非对称加密。这三类算法的作用如下：

- 基于散列函数验证信息的完整性
- 对称加密算法采用协商的秘钥对数据加密
- 非对称加密实现身份认证和秘钥协商

![TLS/SSL](/images/TLS_SSL.png)

## 实现登录

Cookie，Session 和 Token 都是为了能够实现客户端与服务器间的状态维持。其中：

- Cookie 存储于客户端，主要用于一些简短信息的存储；
  HTTP 协议中的 Cookie 包括 Web Cookie 和浏览器 Cookie，它是服务器发送到 Web 浏览器的一小块数据。服务器发送到浏览器的 Cookie，浏览器会进行存储、
- Session 存储于服务器端，维护了客户端的相关信息列表，并且通过一个唯一的身份标识 sessionId 识别各个客户端。
  客户端请求服务端，服务端会为这次请求开辟一块内存空间，这个对象便是 Session 对象，存储结构为 ConcurrentHashMap。Session 弥补了 HTTP 无状态特性，服务器可以利用 Session 存储客户端在同一个会话期间的一些操作记录。
- Token 存储于客户端，其结构紧凑且自带验证功能，很好地解决了 Session 无法水平扩展服务器的局限，实现了服务器无状态但通信具备却状态维持功能。通常使用 JWT（JSON Web Token） 作为 Token 的具体实现方式。

## 单点登录

单点登录是指在同一帐号平台下的多个应用系统中，用户只需登录一次，就可访问所有相互信任的应用系统。比如你在网页中登录了百度云盘，随后你再去贴吧发帖 是不需要二次登录的。

单点登录的本质就是在多个应用系统中共享登录状态。如果用户的登录状态是记录在 Session 中的，要实现共享登录状态，就要先共享 Session，比如可以将 Session 序列化到 Redis 中，让多个应用系统共享同一个 Redis，直接读取 Redis 来获取 Session。

因为不同的应用系统有着不同的域名，尽管 Session 共享了，但是一个企业不同应用的域名不同，依然可能出现跨站 or 跨域。

前端方面的实现方式:

- 父域 Cookie
- 认证中心
- LocalStorage 跨域

## 正反向代理

代理其本质上可以理解为中介。当 A 和 B 不方便进行交互时，往往会引入一个中间角色 C，那么 C 便是中介，便是代理。
正向代理服务器通常位于客户端和服务器之间，类似一个跳板机，通过代理服务器可以访问到目标服务器。
正向代理时，通常，客户端发送对目标服务器的请求，代理服务器在中间将请求转发给目标服务器，并将结果返回给客户端。
反向代理与正向代理恰好相反，代理服务位于服务器端。
对客户端来说，反向代理服务器就好像是目标服务器。反向代理服务器接收客户端发来的请求，然后将其分发到内网的服务器，并将内网服务器返回的结果返回给客户端。
整个过程客户端并不会感知到反向代理后面的服务，也不需要客户端做任何设置，只需要把反向代理服务器当成真正的服务器就行。

## GraphQL 与 Restful 的区别，它有什么优点？

QraphQL 是对后端 REST API 向业务层的聚合与裁剪，REST 更关注对业务细粒度的拆分与重用。
其实就是增加了一个中间层对前端的请求和响应做预处理和后处理，前端的工作少了，后端的工作也没多，却加入了中端的依赖，好处是避免前端和后端的多次远距离的交互。
而 graphql 存在一个很难控制的问题就是查询复杂度。在开发过程中需要把控好解析粒度，而就目前主流关系型数据库，restful api 依旧是最好的选择。graphql 准确的说在查询图结构数据时更有优势，这也是其名称的主意。
